<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="过拟合与欠拟合">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习知识总结">
<meta property="og:url" content="http://yoursite.com/2018/04/02/机器学习知识总结/index.html">
<meta property="og:site_name" content="飞白">
<meta property="og:description" content="过拟合与欠拟合">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/mubiaohanshu.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/duiouwenti.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/tu.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/smo1.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/jueceshu1.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/jueceshu2.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/duishusunshihanshu.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/pingfanggenwucha.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/zhongweishuwucha.png">
<meta property="og:updated_time" content="2018-04-16T12:36:25.662Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习知识总结">
<meta name="twitter:description" content="过拟合与欠拟合">
<meta name="twitter:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/mubiaohanshu.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/04/02/机器学习知识总结/"/>





  <title>机器学习知识总结 | 飞白</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">飞白</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/02/机器学习知识总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="飞白">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习知识总结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-02T08:48:43+08:00">
                2018-04-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h2><a id="more"></a>
<p>拟合程度的好与坏实际上是看模型在训练集上和测试集上的误差，训练集上误差越小同时测试集合训练集误差接近这两个条件都满足时，模型才足够好。<br>而模型的泛化误差又可以分为bias和variance两部分。<br>首先如果我们能够获得所有可能的数据集合，并在这个数据集合上将loss最小化，这样学习到的模型就可以称之为“真实模型”，当然，我们是无论如何都不能获得并训练所有可能的数据的，所以“真实模型”肯定存在，但无法获得，我们的最终目标就是去学习一个模型使其更加接近这个真实模型。而bias和variance分别从两个方面来描述了我们学习到的模型与真实模型之间的差距。Bias是 “用所有可能的训练数据集训练出的所有模型的输出的平均值” 与 “真实模型”的输出值之间的差异；Variance则是“不同的训练数据集训练出的模型”的输出值之间的差异。</p>
<p>关于cross validation中k值对bias和variance的影响，那我就从其他方面来举个例子。假设我们现在有一组训练数据，需要训练一个模型（基于梯度的学习，不包括最近邻等方法）。在训练过程的最初，bias很大，因为我们的模型还没有来得及开始学习，也就是与“真实模型”差距很大。然而此时variance却很小，因为训练数据集（training data）还没有来得及对模型产生影响，所以此时将模型应用于“不同的”训练数据集也不会有太大差异。而随着训练过程的进行，bias变小了，因为我们的模型变得“聪明”了，懂得了更多关于“真实模型”的信息，输出值与真实值之间更加接近了。但是如果我们训练得时间太久了，variance就会变得很大，因为我们除了学习到关于真实模型的信息，还学到了许多具体的，只针对我们使用的训练集（真实数据的子集）的信息。而不同的可能训练数据集（真实数据的子集）之间的某些特征和噪声是不一致的，这就导致了我们的模型在很多其他的数据集上就无法获得很好的效果，也就是所谓的overfitting（过学习）。</p>
<p>1.对于树模型，模型的复杂程度与节点的数量有关<br>防止过拟合：<br>    ①树过于大之前便停止生长<br>    ·每个页中至少需要多少个数据（threshold)<br>    ·如何判断这个阈值（假设检验/P-值）<br>    ②等树生长到足够大之后进行修剪</p>
<p>2.对于数值模型问题，变量的数量与模型的复杂度有关。<br>    二位情况下，两个点可以用一条直线拟合<br>    三维情况下，三个点可以用一个平面拟合<br>    ……<br>    随着维度增加，可以拟合任意数量的点<br>    ①正则化：L1,L2<br>    ②dropout<br>    ③early stop：结合cross validation使用<br>    ④cross validation：数据量较小时，用这个方法比较好。<br>    当K值大的时候， 我们会有更少的Bias(偏差), 更多的Variance。<br>    当K值小的时候， 我们会有更多的Bias(偏差), 更少的Variance。<br>    ⑤增大数据集</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>因为有些数据集是线性不可分的，所以通过引入激活函数来加入非线性因素的。<br>1.sigmod函数<br>也叫Logistic函数，取值范围（0，1），它可以将一个实数映射到(0,1)的区间，可以用来做二分类。<br>sigmoid缺点：<br>激活函数计算量大，反向传播求误差梯度时，求导涉及除法<br>反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练（sigmoid的饱和性）<br>此外，sigmoid函数的输出均大于0，使得输出不是0均值，这称为偏移现象，这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。<br>2.tanh函数<br>与 sigmoid 的区别是，tanh 是 0 均值的，因此实际应用中 tanh 会比 sigmoid 更好，然而，tanh一样具有软饱和性，从而造成梯度消失。<br>3.relu函数<br>ReLU 的优点：<br>Krizhevsky et al. 发现使用 ReLU 得到的 SGD 的收敛速度会比 sigmoid/tanh 快很多。除此之外，当x&lt;0时，ReLU硬饱和，而当x&gt;0时，则不存在饱和问题。所以，ReLU 能够在x&gt;0时保持梯度不衰减，从而缓解梯度消失问题。这让我们能够直接以监督的方式训练深度神经网络，而无需依赖无监督的逐层预训练。</p>
<p>ReLU 的缺点：<br>随着训练的推进，部分输入会落入硬饱和区，导致对应权重无法更新。这种现象被称为“神经元死亡”。与sigmoid类似，ReLU的输出均值也大于0，偏移现象和 神经元死亡会共同影响网络的收敛性。<br>4.softmax函数<br>sigmoid将一个real value映射到（0,1）的区间，用来做二分类。</p>
<p>而 softmax 把一个 k 维的real value向量（a1,a2,a3,a4….）映射成一个（b1,b2,b3,b4….）其中 bi 是一个 0～1 的常数，输出神经元之和为 1.0，所以相当于概率值，然后可以根据 bi 的概率大小来进行多分类的任务。</p>
<h2 id="梯度爆炸与梯度消失"><a href="#梯度爆炸与梯度消失" class="headerlink" title="梯度爆炸与梯度消失"></a>梯度爆炸与梯度消失</h2><p>1.梯度消失：因为通常神经网络所用的激活函数是sigmoid函数，这个函数有个特点，就是能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是f′(x)=f(x)(1−f(x))。因此两个0到1之间的数相乘，得到的结果就会变得很小了。神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新，这就是梯度消失。<br>2.梯度爆炸：梯度爆炸就是由于初始化权值过大，前面层会比后面层变化的更快，就会导致权值越来越大，梯度爆炸的现象就发生了。</p>
<p>在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。</p>
<p>网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。<br>3.如何确定是否出现梯度爆炸？</p>
<p>训练过程中出现梯度爆炸会伴随一些细微的信号，如：</p>
<p>模型无法从训练数据中获得更新（如低损失）。</p>
<p>模型不稳定，导致更新过程中的损失出现显著变化。</p>
<p>训练过程中，模型损失变成 NaN。<br>4.如何修复梯度爆炸问题？<br>①重新设计网络模型<br>②使用Relu激活函数<br>③使用长短期记忆网络<br>④使用梯度截断<br>⑤使用权重正则化：检查网络权重的大小，并惩罚产生较大权重值的损失函数</p>
<h2 id="分类算法（LR-SVM-朴素贝叶斯-决策树，RF-GBDT-Adaboost-xgboost，KNN"><a href="#分类算法（LR-SVM-朴素贝叶斯-决策树，RF-GBDT-Adaboost-xgboost，KNN" class="headerlink" title="分类算法（LR,SVM,朴素贝叶斯,决策树，RF,GBDT,Adaboost,xgboost，KNN)"></a>分类算法（LR,SVM,朴素贝叶斯,决策树，RF,GBDT,Adaboost,xgboost，KNN)</h2><h3 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h3><p>1.面对一个回归或者分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏。逻辑回归函数（Sigmod)非凸函数，是用最大似然估计进行模型参数的求解。<br>2.Regression 常规步骤<br>寻找h函数（即预测函数）<br>构造J函数（损失函数）<br>想办法使得J函数最小并求得回归参数（θ）<br>3.优缺点<br>优点：<br>1）速度快，适合二分类问题<br>2）简单易于理解，直接看到各个特征的权重<br>3）容易使用和解释，直接看到各个特征的权重。<br>缺点：<br>对数据和场景的适应能力有局限性，不如决策树算法适应性那么强</p>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>SVM是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大是它有别于感知机）</p>
<p>（1）当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；</p>
<p>（2）当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；</p>
<p>（3）当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。</p>
<p>注：以上各SVM的数学推导应该熟悉：硬间隔最大化（几何间隔）—学习的对偶问题—软间隔最大化（引入松弛变量）—非线性支持向量机（核技巧）。</p>
<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>1.优点：可以解决小样本下机器学习的问题。提高泛化性能。避免神经网络结构选择和局部极小的问题。<br>2.缺点：缺失数据敏感。内存消耗大，难以解释。</p>
<h4 id="SVM为什么采用间隔最大化？"><a href="#SVM为什么采用间隔最大化？" class="headerlink" title="SVM为什么采用间隔最大化？"></a>SVM为什么采用间隔最大化？</h4><p>当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。</p>
<p>感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。</p>
<p>线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。</p>
<p>然后应该借此阐述，几何间隔，函数间隔，及从函数间隔—&gt;求解最小化1/2 ||w||^2 时的w和b。即线性可分支持向量机学习算法—最大间隔法的由来。</p>
<h4 id="线性SVM"><a href="#线性SVM" class="headerlink" title="线性SVM"></a>线性SVM</h4><p>加入了松弛因子。<br>SVM目标函数为<img src="/2018/04/02/机器学习知识总结/mubiaohanshu.png" alt="lbxx"><br>而损失函数就是后面的Ei的和<br>SVM实际上可以看作是hinge损失和l2正则</p>
<h4 id="解释对偶的概念。"><a href="#解释对偶的概念。" class="headerlink" title="解释对偶的概念。"></a>解释对偶的概念。</h4><p>极小极大问题转化为极大极小问题。SVM拉格朗日乘子法前半部分1/2||w^2||为二次函数，凸函数，后半部分线性的，所以转为为对偶问题是充分条件。一般情况下极小极大大于等于极大极小。</p>
<h4 id="为什么要将求解SVM的原始问题转换为其对偶问题？"><a href="#为什么要将求解SVM的原始问题转换为其对偶问题？" class="headerlink" title="为什么要将求解SVM的原始问题转换为其对偶问题？"></a>为什么要将求解SVM的原始问题转换为其对偶问题？</h4><p>一、是对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）</p>
<p>二、自然引入核函数，进而推广到非线性分类问题。</p>
<h4 id="为什么SVM要引入核函数？"><a href="#为什么SVM要引入核函数？" class="headerlink" title="为什么SVM要引入核函数？"></a>为什么SVM要引入核函数？</h4><p>当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。</p>
<p>引入映射后的对偶问题：<br><img src="/2018/04/02/机器学习知识总结/duiouwenti.png" alt="lbxx"><br>在学习预测中，只定义核函数K(x,y)，而不是显式的定义映射函数ϕ。因为特征空间维数可能很高，甚至可能是无穷维，因此直接计算ϕ(x)·ϕ(y)是比较困难的。相反，直接计算K(x,y)比较容易（即直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果）。</p>
<p>核函数的定义：K(x,y)=&lt;ϕ(x),ϕ(y)&gt;，即在特征空间的内积等于它们在原始样本空间中通过核函数K计算的结果。</p>
<p>除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。</p>
<h4 id="为什么SVM对缺失数据敏感？"><a href="#为什么SVM对缺失数据敏感？" class="headerlink" title="为什么SVM对缺失数据敏感？"></a>为什么SVM对缺失数据敏感？</h4><p>这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM没有处理缺失值的策略（决策树有）。而SVM希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。</p>
<h4 id="SVM如何处理多分类问题？"><a href="#SVM如何处理多分类问题？" class="headerlink" title="SVM如何处理多分类问题？"></a>SVM如何处理多分类问题？</h4><p>一般有两种做法：一种是直接法，直接在目标函数上修改，将多个分类面的参数求解合并到一个最优化问题里面。看似简单但是计算量却非常的大。</p>
<p>另外一种做法是间接法：对训练器进行组合。其中比较典型的有一对一，和一对多。</p>
<p>一对多，就是对每个类都训练出一个分类器，由svm是二分类，所以将此而分类器的两类设定为目标类为一类，其余类为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，那个分类器的概率高，那么这个样本就属于哪一类。这种方法效果不太好，bias比较高。</p>
<p>svm一对一法（one-vs-one），针对任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k) 个分类器，这样当有一个新的样本要来的时候，用这C(2,k) 个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。</p>
<h4 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h4><p>KKT条件的定理是什么呢？就是如果一个优化问题在转变完后变成<br>L(x,α,β)=f(x)+∑αigi(x)+∑βihi(x)<br>L(x,α,β)=f(x)+∑αigi(x)+∑βihi(x)</p>
<p>其中g是不等式约束，h是等式约束（像上面那个只有不等式约束，也可能有等式约束）。那么KKT条件就是函数的最优值必定满足下面条件：<br>(1) L对各个x求导为零；<br>(2) h(x)=0;<br>(3) ∑αigi(x)=0，αi≥0</p>
<h4 id="SMO-序列最小最优化算法"><a href="#SMO-序列最小最优化算法" class="headerlink" title="SMO(序列最小最优化算法)"></a>SMO(序列最小最优化算法)</h4><p>1.定义：用于求解SVM对偶问题的最优解的二次规划方法。<br>2.<img src="/2018/04/02/机器学习知识总结/tu.png" alt="lbxx"><br>为了求解N个参数(α1,α2,α3,…,αN)，首先想到的是坐标上升的思路，例如求解α1,可以固定其他N-1个参数，可以看成关于α1的一元函数求解，但是注意到上述问题的等式约束条件<br><img src="/2018/04/02/机器学习知识总结/smo1.png" alt="lbxx">当固定其他参数时，参数α1也被固定，因此此种方法不可用。<br>SMO算法选择同时优化两个参数，固定其他N-2个参数，假设选择的变量为α1,α2,固定其他参数α3,α4,…,αN,由于参数α3,α4,…,αN的固定，可以简化目标函数为只关于α1,α2的二元函数，Constant表示常数项(不包含变量α1,α2的项)。 </p>
<h4 id="python实现代码"><a href="#python实现代码" class="headerlink" title="python实现代码"></a>python实现代码</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">import random </span><br><span class="line"></span><br><span class="line">def loadDataSet(fileName):  <span class="comment">#构建数据库和标记库</span></span><br><span class="line">    dataMat = []; labelMat = [] </span><br><span class="line">    fr = open(fileName) </span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines(): </span><br><span class="line">        lineArr = line.strip().split(<span class="string">'\t'</span>) </span><br><span class="line">        dataMat.append([<span class="built_in">float</span>(lineArr[0]), <span class="built_in">float</span>(lineArr[1])]) </span><br><span class="line">        labelMat.append(<span class="built_in">float</span>(lineArr[2]))  <span class="comment">#只有一列</span></span><br><span class="line">    <span class="built_in">return</span> dataMat, labelMat </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def selectJrand(i, m):  <span class="comment">#生成一个随机数</span></span><br><span class="line">    j=i </span><br><span class="line">    <span class="keyword">while</span>(j==i): </span><br><span class="line">        j=int(random.uniform(0, m))  <span class="comment">#生成一个[0, m]的随机数，int转换为整数。注意，需要import random</span></span><br><span class="line">    <span class="built_in">return</span> j </span><br><span class="line"></span><br><span class="line">def clipAlpha(aj, H, L):  <span class="comment">#阈值函数</span></span><br><span class="line">    <span class="keyword">if</span> aj&gt;H: </span><br><span class="line">        aj=H </span><br><span class="line">    <span class="keyword">if</span> aj&lt;L: </span><br><span class="line">        aj=L </span><br><span class="line">    <span class="built_in">return</span> aj </span><br><span class="line"></span><br><span class="line">def smoSimple(dataMatIn, classLabels, C, toler, maxIter): </span><br><span class="line">    dataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose()</span><br><span class="line">    b = 0; m,n = shape(dataMatrix)</span><br><span class="line">    alphas = mat(zeros((m,1)))</span><br><span class="line">    iter = 0</span><br><span class="line">    <span class="keyword">while</span>(iter&lt;maxIter):  <span class="comment">#迭代次数</span></span><br><span class="line">        alphaPairsChanged=0 </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):  <span class="comment">#在数据集上遍历每一个alpha</span></span><br><span class="line">            <span class="comment">#print alphas </span></span><br><span class="line">            <span class="comment">#print labelMat</span></span><br><span class="line">            fXi = <span class="built_in">float</span>(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b</span><br><span class="line">            <span class="comment">#fXi=float(np.multiply(alphas, labelMat).T*dataMatrix*dataMatrix[i, :].T)+b  #.T也是转置</span></span><br><span class="line">            Ei=fXi-float(labelMat[i]) </span><br><span class="line">            <span class="keyword">if</span>((labelMat[i]*Ei&lt;-toler) and (alphas[i]&lt;C)) or ((labelMat[i]*Ei&gt;toler) and (alphas[i]&gt;0)): </span><br><span class="line">                j=selectJrand(i, m)  <span class="comment">#从m中选择一个随机数，第2个alpha j</span></span><br><span class="line">                fXj=<span class="built_in">float</span>(multiply(alphas, labelMat).T*dataMatrix*dataMatrix[j, :].T)+b </span><br><span class="line">                Ej=fXj-float(labelMat[j]) </span><br><span class="line">                </span><br><span class="line">                alphaIold=alphas[i].copy()  <span class="comment">#复制下来，便于比较</span></span><br><span class="line">                alphaJold=alphas[j].copy() </span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span>(labelMat[i]!=labelMat[j]):  <span class="comment">#开始计算L和H</span></span><br><span class="line">                    L=max(0, alphas[j]-alphas[i]) </span><br><span class="line">                    H=min(C, C+alphas[j]-alphas[i]) </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    L=max(0, alphas[j]+alphas[i]-C) </span><br><span class="line">                    H=min(C, alphas[j]+alphas[i]) </span><br><span class="line">                <span class="keyword">if</span> L==H: </span><br><span class="line">                    <span class="built_in">print</span> <span class="string">'L==H'</span> </span><br><span class="line">                    <span class="built_in">continue</span> </span><br><span class="line">                </span><br><span class="line">                <span class="comment">#eta是alphas[j]的最优修改量，如果eta为零，退出for当前循环</span></span><br><span class="line">                eta=2.0*dataMatrix[i, :]*dataMatrix[j, :].T-\</span><br><span class="line">                    dataMatrix[i, :]*dataMatrix[i, :].T-\</span><br><span class="line">                    dataMatrix[j, :]*dataMatrix[j, :].T </span><br><span class="line">                <span class="keyword">if</span> eta&gt;=0: </span><br><span class="line">                    <span class="built_in">print</span> <span class="string">'eta&gt;=0'</span> </span><br><span class="line">                    <span class="built_in">continue</span> </span><br><span class="line">                alphas[j]-=labelMat[j]*(Ei-Ej)/eta  <span class="comment">#调整alphas[j] </span></span><br><span class="line">                alphas[j]=clipAlpha(alphas[j], H, L)  </span><br><span class="line">                <span class="keyword">if</span>(abs(alphas[j]-alphaJold)&lt;0.00001):  <span class="comment">#如果alphas[j]没有调整</span></span><br><span class="line">                    <span class="built_in">print</span> <span class="string">'j not moving enough'</span> </span><br><span class="line">                    <span class="built_in">continue</span> </span><br><span class="line">                alphas[i]+=labelMat[j]*labelMat[i]*(alphaJold-alphas[j])  <span class="comment">#调整alphas[i]</span></span><br><span class="line">                b1=b-Ei-labelMat[i]*(alphas[i]-alphaIold)*\</span><br><span class="line">                    dataMatrix[i, :]*dataMatrix[i, :].T-\</span><br><span class="line">                    labelMat[j]*(alphas[j]-alphaJold)*\</span><br><span class="line">                    dataMatrix[i, :]*dataMatrix[j, :].T </span><br><span class="line">                b2=b-Ej-labelMat[i]*(alphas[i]-alphaIold)*\</span><br><span class="line">                    dataMatrix[i, :]*dataMatrix[j, :].T-\</span><br><span class="line">                    labelMat[j]*(alphas[j]-alphaJold)*\</span><br><span class="line">                    dataMatrix[j, :]*dataMatrix[j, :].T </span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span>(0&lt;alphas[i]) and (C&gt;alphas[i]): </span><br><span class="line">                    b=b1 </span><br><span class="line">                <span class="keyword">elif</span>(0&lt;alphas[j]) and (C&gt;alphas[j]): </span><br><span class="line">                    b=b2 </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    b=(b1+b2)/2.0 </span><br><span class="line">                alphaPairsChanged+=1 </span><br><span class="line">                </span><br><span class="line">                <span class="built_in">print</span> <span class="string">'iter: %d i: %d, pairs changed %d'</span> %(iter, i, alphaPairsChanged) </span><br><span class="line">        <span class="keyword">if</span>(alphaPairsChanged==0): </span><br><span class="line">            iter+=1 </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            iter=0 </span><br><span class="line">        <span class="built_in">print</span> <span class="string">'iteration number: %d'</span> %iter</span><br><span class="line">    <span class="built_in">return</span> b, alphas </span><br><span class="line"></span><br><span class="line">                </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>: </span><br><span class="line">    dataArr, labelArr=loadDataSet(<span class="string">'testSet.txt'</span>) </span><br><span class="line">    b, alphas=smoSimple(dataArr, labelArr, 0.6, 0.001, 40)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span> b, alphas</span><br></pre></td></tr></table></figure>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>1.基本描述：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此分类项属于哪个类别。<br>2.优点：方法简单，分类准确率高，速度快，所需估计的参数少，对于缺失数据不敏感。<br>3.缺点：假设一个属性对定类的影响独立于其他的属性值，这往往并不成立。<br>4.</p>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="基本描述"><a href="#基本描述" class="headerlink" title="基本描述"></a>基本描述</h4><p>决策树是一种简单但广泛使用的分类器，它通过训练数据构建决策树，对未知的数据进行分类。决策树的每个内部节点表示在一个属性上的测试，每个分枝代表该测试的一个输出，而每个叶结点存放着一个类标号。</p>
<p>在决策树算法中，ID3基于信息增益作为属性选择的度量，C4.5基于信息增益比作为属性选择的度量，CART基于基尼指数作为属性选择的度量。<br><img src="/2018/04/02/机器学习知识总结/jueceshu1.png" alt="lbxx"></p>
<h4 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h4><p>优点：不需要任何领域知识或参数假设。适合高维数据。简单易于理解，可解释性强。短时间内处理大量数据，得到可行且效果较好的结果(训练速度快)。<br>缺点：对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。<br>易于过拟合。忽略属性之间的相关性。</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>决策树的损失函数通常是正则化的极大似然函数，学习的策略是以损失函数为目标函数的最小化<br>由于这个最小化问题是一个NP完全问题，现实中，我们通常采用启发式算法（这里，面试官可能会问什么是启发式算法，要有准备，SMO算法就是启发式算法）来近似求解这一最优化问题，得到的决策树是次最优的。<br>该启发式算法可分为三步：<br>·特征选择<br>·模型生成<br>·决策树的剪枝<br>从总体上看，这三个步骤就是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程，这个过程就是划分特征空间，构建决策树的过程。</p>
<h4 id="递归的终止条件是什么呢？"><a href="#递归的终止条件是什么呢？" class="headerlink" title="递归的终止条件是什么呢？"></a>递归的终止条件是什么呢？</h4><p>通常有两个终止条件，一是所有训练数据子集被基本正确分类。二是没有合适的特征可选，即可用特征为0，或者可用特征的信息增益或信息增益比都很小了。</p>
<h4 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h4><p>1.一般剪枝<br><img src="/2018/04/02/机器学习知识总结/jueceshu2.png" alt="lbxx"><br>C(T)表示决策树T对训练数据集的预测误差，为经验熵，|T|表示决策树的叶节点个数，即模型复杂度。α是权衡因子。给定α，通过不断剪枝来减小损失。<br>2.CART剪枝算法<br>CART剪枝算法由两步组成：首先从生成算法产生的决策树T0T0底端开始不断剪枝，直到T0T0的根结点，形成一个子树序列{T0,T1,…,Tn}{T0,T1,…,Tn}；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。<br>为了得到树T的所有子序列{T0,T1,…,Tn}{T0,T1,…,Tn}，我们需要从底自上，每次只进行一次剪枝操作，那么进行剪枝操作后，该子序列应该是当前参数的最优子树。且应该是根据所剪的那个结点来计算参数αα。</p>
<h4 id="决策树如何处理连续值"><a href="#决策树如何处理连续值" class="headerlink" title="决策树如何处理连续值"></a>决策树如何处理连续值</h4><p>体的思路如下，比如m个样本的连续特征A有m个，从小到大排列为a1,a2,…,ama1,a2,…,am,则CART算法取相邻两样本值的中位数，一共取得m-1个划分点，其中第i个划分点Ti表示Ti表示为：Ti=ai+ai+12Ti=ai+ai+12。对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为atat,则小于atat的值为类别1，大于atat的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。</p>
<p>对于CART分类树离散值的处理问题，采用的思路是不停的二分离散特征。<br>ID3或者C4.5，如果某个特征A被选取建立决策树节点，如果它有A1,A2,A3三种类别，我们会在决策树上一下建立一个三叉的节点。这样导致决策树是多叉树。但是CART分类树使用的方法不同，他采用的是不停的二分</p>
<h3 id="RF"><a href="#RF" class="headerlink" title="RF"></a>RF</h3><p>1.数据的随机选取：<br>第一，从原始的数据集中采取有放回的抽样，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。<br>第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。<br>2.待选特征的随机选取：<br>与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征（因此最好可以保证特征之间没有较强的相关性），之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。<br>2.随机森林的超参数：<br>“n_estimators”：树的个数<br>“max_features”：每个树中可拥有的最大特征数量<br>min_sample_leaf”：决定叶子的数量<br>3.相比于决策树，随机森林的投票机制可以防止过拟合。<br>4.优缺点：<br>优点：<br>在数据集上表现良好，相对于其他算法有较大的优势（训练速度、预测准确度）；<br>能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性；<br>容易做成并行化方法。<br>缺点：在噪声较大的分类或者回归问题上会过拟合。</p>
<h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><p>1.基本描述：通过计算每个训练样例到待分类样品的距离，取和待分类样品距离最近的K个训练样例，K个样品中哪个类别的训练样例占多数，则待分类样品就属于哪个类别。<br>2.优缺点<br>优点：适用于样本容量比较大的分类问题<br>缺点：计算量太大，对于样本量较小的分类问题，会产生误分。</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p>1.基本描述：<br>首先gbdt 是通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法。<br>通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。<br>2.如何选择特征：原始的gbdt的做法非常的暴力，首先遍历每个特征，然后对每个特征遍历它所有可能的切分点，找到最优特征 m 的最优切分点 j。<br>3.优缺点：<br>优点：<br>它能灵活的处理各种类型的数据；<br>在相对较少的调参时间下，预测的准确度较高。<br>缺点：当然由于它是Boosting，因此基学习器之前存在串行关系，难以并行训练数据。<br>4.GBDT用于分类<br>第一步 我们在训练的时候，是针对样本 X 每个可能的类都训练一个分类回归树。举例说明，目前样本有三类，也就是 K = 3。样本 x 属于 第二类。那么针对该样本 x 的分类结果，其实我们可以用一个 三维向量 [0,1,0] 来表示。0表示样本不属于该类，1表示样本属于该类。由于样本已经属于第二类了，所以第二类对应的向量维度为1，其他位置为0。<br><a href="https://www.cnblogs.com/ModifyRong/p/7744987.html" target="_blank" rel="noopener">更多</a></p>
<h3 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h3><p>1.基本描述<br>（1）xgboost在代价函数里自带加入了正则项，用于控制模型的复杂度。<br>（2）xgboost在进行节点的分裂时，支持各个特征多线程进行增益计算，因此算法更快，准确率也相对高一些。<br>2.相比于GBDT<br>①传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。<br>②传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。<br>③xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。<br>④Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）<br>⑤列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。<br>⑥对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。<br>⑦xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。<br>⑧可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。</p>
<h3 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h3><h3 id="GBDT-和随机森林的区别？"><a href="#GBDT-和随机森林的区别？" class="headerlink" title="GBDT 和随机森林的区别？"></a>GBDT 和随机森林的区别？</h3><h3 id="如何判断函数凸或非凸？什么是凸优化？"><a href="#如何判断函数凸或非凸？什么是凸优化？" class="headerlink" title="如何判断函数凸或非凸？什么是凸优化？"></a>如何判断函数凸或非凸？什么是凸优化？</h3><h3 id="如何解决类别不平衡问题？"><a href="#如何解决类别不平衡问题？" class="headerlink" title="如何解决类别不平衡问题？"></a>如何解决类别不平衡问题？</h3><h3 id="如何进行特征选择？"><a href="#如何进行特征选择？" class="headerlink" title="如何进行特征选择？"></a>如何进行特征选择？</h3><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><h3 id="分类评价指标"><a href="#分类评价指标" class="headerlink" title="分类评价指标"></a>分类评价指标</h3><p>1.准确率<br>2.平均准确率<br>计算每个类别的准确率最后取平均<br>3.对数损失函数<br>在分类输出中，若输出不再是0-1，而是实数值，即属于每个类别的概率，那么可以使用Log-loss对分类结果进行评价。这个输出概率表示该记录所属的其对应的类别的置信度。比如如果样本本属于类别0，但是分类器则输出其属于类别1的概率为0.51，那么这种情况认为分类器出错了。该概率接近了分类器的分类的边界概率0.5。Log-loss是一个软的分类准确率度量方法，使用概率来表示其所属的类别的置信度。Log-loss具体的数学表达式为：<br><img src="/2018/04/02/机器学习知识总结/duishusunshihanshu.png" alt="lbxx"><br>其中，yiyi是指第ii个样本所属的真实类别0或者1，pipi表示第ii个样本属于类别1的概率，这样上式中的两个部分对于每个样本只会选择其一，因为有一个一定为0，当预测与实际类别完全匹配时，则两个部分都是0，其中假定0log0=0。<br>4.精确率，召回率，F-measure<br>5.AUC：即曲线下的面积，这条曲线便是ROC曲线</p>
<h3 id="回归评价指标"><a href="#回归评价指标" class="headerlink" title="回归评价指标"></a>回归评价指标</h3><p>与分类不同的是，回归是对连续的实数值进行预测，即输出值是连续的实数值，而分类中是离散值。例如，给你历史股票价格，公司与市场的一些信息，需要你去预测将来一段时间内股票的价格走势。那么这个任务便是回归任务。对于回归模型的评价指标主要有以下几种：<br>1.RMSE:回归模型中最常用的评价模型便是RMSE（root mean square error，平方根误差），其又被称为RMSD（root mean square deviation），其定义如下：<br><img src="/2018/04/02/机器学习知识总结/pingfanggenwucha.png" alt="lbxx"><br>其中，yi是第i个样本的真实值，yi^是第ii个样本的预测值，n是样本的个数。该评价指标使用的便是欧式距离。<br>RMSE虽然广为使用，但是其存在一些缺点，因为它是使用平均误差，而平均值对异常点（outliers）较敏感，如果回归器对某个点的回归值很不理性，那么它的误差则较大，从而会对RMSE的值有较大影响，即平均值是非鲁棒的。<br>2.Quantiles of Errors<br>在现实数据中，往往会存在异常点，并且模型可能对异常点拟合得并不好，因此提高评价指标的鲁棒性至关重要，于是可以使用中位数来替代平均数，如MAPE：<br><img src="/2018/04/02/机器学习知识总结/zhongweishuwucha.png" alt="lbxx"><br>MAPE是一个相对误差的中位数，当然也可以使用别的分位数。<br>有时我们可以使用相对误差不超过设定的值来计算平均误差，如当|yi−yi^|/yi超过100%（具体的值要根据问题的实际情况）则认为其是一个异常点，，从而剔除这个异常点，将异常点剔除之后，再计算平均误差或者中位数误差来对模型进行评价。</p>
<h3 id="排序评价指标"><a href="#排序评价指标" class="headerlink" title="排序评价指标"></a>排序评价指标</h3><p>1.精确率-召回率-F-measure</p>
<h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><p>1.什么是模型超参数？<br>模型超参数是模型外部的配置，其值不能从数据估计得到。<br>2.具体特征有：<br>模型超参数常应用于估计模型参数的过程中。<br>模型超参数通常由实践者直接指定。<br>模型超参数通常可以使用启发式方法来设置。<br>模型超参数通常根据给定的预测建模问题而调整。<br>3.例子：<br>训练神经网络的学习速率。<br>支持向量机的C和sigma超参数。<br>k邻域中的k。<br>4.网格搜索，随机搜索，贝叶斯优化<br><a href="https://blog.csdn.net/gzj533/article/details/77734310" target="_blank" rel="noopener">更多</a></p>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><h2 id="局部敏感哈希"><a href="#局部敏感哈希" class="headerlink" title="局部敏感哈希"></a>局部敏感哈希</h2><h2 id="数据偏斜"><a href="#数据偏斜" class="headerlink" title="数据偏斜"></a>数据偏斜</h2><p>基于算法：<br>在算法中提高少数类别样例的权重，对少数类别样本错误分类的代价高于多数类别样本的错分。</p>
<p>基于数据：<br>两种。一是过采样，在数据集中增加少数类别样例。二是下采样，减少多数类别的样本个数。</p>
<h3 id="重采样"><a href="#重采样" class="headerlink" title="重采样"></a>重采样</h3><p>上采样<br>下采样</p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>如何解决L1求导困难</p>
<h2 id="bagging和boosting"><a href="#bagging和boosting" class="headerlink" title="bagging和boosting"></a>bagging和boosting</h2><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><h2 id="SVD，PCA-LDA-FA"><a href="#SVD，PCA-LDA-FA" class="headerlink" title="SVD，PCA,LDA,FA"></a>SVD，PCA,LDA,FA</h2><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><h2 id="DBSCAN算法"><a href="#DBSCAN算法" class="headerlink" title="DBSCAN算法"></a>DBSCAN算法</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>1.Eps邻域：给定对象半径Eps内的邻域称为该对象的Eps邻域<br>2.核心点：若对象的Eps邻域内至少包含minpts个点，即为核心点<br>3.边界点：不是核心点，但在核心点的邻域内<br>4.密度直达：给定一个对象集合D，如果p在q的Eps邻域内，而q是一个核心对象，则称对象p从对象q出发时是直接密度可达的;<br>5.密度可达：如果存在一个对象链  p1, …,pi,.., pn，满足p1 = p 和pn = q，pi是从pi+1关于Eps和MinPts直接密度可达的，则对象p是从对象q关于Eps和MinPts密度可达的;<br>6.密度相连：如果存在对象O∈D，使对象p和q都是从O关于Eps和MinPts密度可达的，那么对象p到q是关于Eps和MinPts密度相连的。</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>1.找核心点集，边界点和噪声点<br>2.删除噪声点<br>3.迭代，从核心点找密度直达的对向，可能包含一些密度可达簇的合并<br>4.当没有新的点添加到任何促时，结束</p>
<h3 id="优缺点-2"><a href="#优缺点-2" class="headerlink" title="优缺点"></a>优缺点</h3><p>优点：</p>
<ol>
<li><p>与K-means方法相比，DBSCAN不需要事先知道要形成的簇类的数量。</p>
</li>
<li><p>与K-means方法相比，DBSCAN可以发现任意形状的簇类。</p>
</li>
<li><p>同时，DBSCAN能够识别出噪声点。对离群点有较好的鲁棒性，甚至可以检测离群点。</p>
</li>
</ol>
<p>4.DBSCAN对于数据库中样本的顺序不敏感，即Pattern的输入顺序对结果的影响不大。但是，对于处于簇类之间边界样本，可能会根据哪个簇类优先被探测到而其归属有所摆动。</p>
<p>5.DBSCAN被设计与数据库一同使用，可以加速区域的查询。例如 使用R*树<br>缺点：</p>
<ol>
<li><p>DBScan不能很好反映高维数据。</p>
</li>
<li><p>DBScan不能很好反映数据集以变化的密度。</p>
</li>
</ol>
<p>3.由于DBSCAN算法直接对整个数据集进行操作，并且在聚类之前需要建立相应的R*树，并绘制k-dist图，因此算法所需的内存空间和I/O消耗都相当可观。在计算资源有限而数据量又非常庞大的情况下，DBSCAN算法的效率将受到很大影响。（DBSCAN算法将区域查询得到的所有未被处理过的点都作为种子点，留待下一步扩展处理。对于大规模数据集中的较大类而言，这种策略会使种子点的数目不断膨胀，算法所需的内存空间也会快速增加。）</p>
<p>4.由于DBSCAN算法使用了全局性表征密度的参数，因此当各个类的密度不均匀，或类间的距离相差很大时，聚类的质量较差。(当各个类的密度不均匀、或类间的距离相差很大时，如果根据密度较高的类选取较小的Eps值，那么密度相对较低的类中的对象Eps 邻域中的点数将小Minpts，则这些点将会被错当成边界点，从而不被用于所在类的进一步扩展，因此导致密度较低的类被划分成多个性质相似的类。与此相反，如果根据密度较低的类来选取较大的Eps值，则会导致离得较近而密度较大的类被合并，而它们之间的差异被忽略。所以在上述情况下，很难选取一个合适的全局Eps值来获得比较准确的聚类结果。)</p>
<p>5.DBSCAN不是完全确定的，边界点从不同的簇中获得，可以使不同簇的一部分，取决于数据处理。</p>
<p>6.DBSCAN的质量取决于regionQuery(P,Eps)函数中距离的测量。最常用的距离度量是欧式距离，尤其是在高维数据中，由于所谓的维数灾难，这种度量基本上是无用的，很难为E找到一个恰当的值。虽然目前有一些基于欧式距离的算法，但是如果不能对数据和规模有很好的了解，也很难找一个有意义的距离阈值E。</p>
<p>7.当密度差异大时，由于选取的MinPts-Eps组合不能同时适合所有的簇，DBSACN不能很好的进行数据聚类。（缺点4）</p>
<p>8.输入参数敏感,确定参数Eps , MinPts困难 ,若选取不当 ,将造成聚类质量下降。</p>
<p>9.由于经典的DBSCAN算法中参数Eps和MinPts在聚类过程中是不变的，使得该算法难以适应密度不均匀的数据集．</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/01/算法/" rel="next" title="算法">
                <i class="fa fa-chevron-left"></i> 算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/05/深度学习知识总结/" rel="prev" title="深度学习知识总结">
                深度学习知识总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John</p>
              <p class="site-description motion-element" itemprop="description">学无止境</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#过拟合与欠拟合"><span class="nav-number">1.</span> <span class="nav-text">过拟合与欠拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数"><span class="nav-number">2.</span> <span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度爆炸与梯度消失"><span class="nav-number">3.</span> <span class="nav-text">梯度爆炸与梯度消失</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分类算法（LR-SVM-朴素贝叶斯-决策树，RF-GBDT-Adaboost-xgboost，KNN"><span class="nav-number">4.</span> <span class="nav-text">分类算法（LR,SVM,朴素贝叶斯,决策树，RF,GBDT,Adaboost,xgboost，KNN)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LR"><span class="nav-number">4.1.</span> <span class="nav-text">LR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM"><span class="nav-number">4.2.</span> <span class="nav-text">SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#定义"><span class="nav-number">4.2.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优缺点"><span class="nav-number">4.2.2.</span> <span class="nav-text">优缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM为什么采用间隔最大化？"><span class="nav-number">4.2.3.</span> <span class="nav-text">SVM为什么采用间隔最大化？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性SVM"><span class="nav-number">4.2.4.</span> <span class="nav-text">线性SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解释对偶的概念。"><span class="nav-number">4.2.5.</span> <span class="nav-text">解释对偶的概念。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么要将求解SVM的原始问题转换为其对偶问题？"><span class="nav-number">4.2.6.</span> <span class="nav-text">为什么要将求解SVM的原始问题转换为其对偶问题？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么SVM要引入核函数？"><span class="nav-number">4.2.7.</span> <span class="nav-text">为什么SVM要引入核函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么SVM对缺失数据敏感？"><span class="nav-number">4.2.8.</span> <span class="nav-text">为什么SVM对缺失数据敏感？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM如何处理多分类问题？"><span class="nav-number">4.2.9.</span> <span class="nav-text">SVM如何处理多分类问题？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KKT条件"><span class="nav-number">4.2.10.</span> <span class="nav-text">KKT条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SMO-序列最小最优化算法"><span class="nav-number">4.2.11.</span> <span class="nav-text">SMO(序列最小最优化算法)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#python实现代码"><span class="nav-number">4.2.12.</span> <span class="nav-text">python实现代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#朴素贝叶斯"><span class="nav-number">4.3.</span> <span class="nav-text">朴素贝叶斯</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树"><span class="nav-number">4.4.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本描述"><span class="nav-number">4.4.1.</span> <span class="nav-text">基本描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优缺点-1"><span class="nav-number">4.4.2.</span> <span class="nav-text">优缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#损失函数"><span class="nav-number">4.4.3.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#递归的终止条件是什么呢？"><span class="nav-number">4.4.4.</span> <span class="nav-text">递归的终止条件是什么呢？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#剪枝"><span class="nav-number">4.4.5.</span> <span class="nav-text">剪枝</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树如何处理连续值"><span class="nav-number">4.4.6.</span> <span class="nav-text">决策树如何处理连续值</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RF"><span class="nav-number">4.5.</span> <span class="nav-text">RF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KNN"><span class="nav-number">4.6.</span> <span class="nav-text">KNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT"><span class="nav-number">4.7.</span> <span class="nav-text">GBDT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xgboost"><span class="nav-number">4.8.</span> <span class="nav-text">xgboost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adaboost"><span class="nav-number">4.9.</span> <span class="nav-text">Adaboost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT-和随机森林的区别？"><span class="nav-number">4.10.</span> <span class="nav-text">GBDT 和随机森林的区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何判断函数凸或非凸？什么是凸优化？"><span class="nav-number">4.11.</span> <span class="nav-text">如何判断函数凸或非凸？什么是凸优化？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何解决类别不平衡问题？"><span class="nav-number">4.12.</span> <span class="nav-text">如何解决类别不平衡问题？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何进行特征选择？"><span class="nav-number">4.13.</span> <span class="nav-text">如何进行特征选择？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#评价指标"><span class="nav-number">5.</span> <span class="nav-text">评价指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分类评价指标"><span class="nav-number">5.1.</span> <span class="nav-text">分类评价指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#回归评价指标"><span class="nav-number">5.2.</span> <span class="nav-text">回归评价指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#排序评价指标"><span class="nav-number">5.3.</span> <span class="nav-text">排序评价指标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#超参数"><span class="nav-number">6.</span> <span class="nav-text">超参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#聚类"><span class="nav-number">7.</span> <span class="nav-text">聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#局部敏感哈希"><span class="nav-number">8.</span> <span class="nav-text">局部敏感哈希</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据偏斜"><span class="nav-number">9.</span> <span class="nav-text">数据偏斜</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#重采样"><span class="nav-number">9.1.</span> <span class="nav-text">重采样</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征选择"><span class="nav-number">10.</span> <span class="nav-text">特征选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化"><span class="nav-number">11.</span> <span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bagging和boosting"><span class="nav-number">12.</span> <span class="nav-text">bagging和boosting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化方法"><span class="nav-number">13.</span> <span class="nav-text">优化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SVD，PCA-LDA-FA"><span class="nav-number">14.</span> <span class="nav-text">SVD，PCA,LDA,FA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征工程"><span class="nav-number">15.</span> <span class="nav-text">特征工程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DBSCAN算法"><span class="nav-number">16.</span> <span class="nav-text">DBSCAN算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#概念"><span class="nav-number">16.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原理"><span class="nav-number">16.2.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优缺点-2"><span class="nav-number">16.3.</span> <span class="nav-text">优缺点</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
