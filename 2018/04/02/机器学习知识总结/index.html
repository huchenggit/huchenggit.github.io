<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="过拟合与欠拟合">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习知识总结">
<meta property="og:url" content="http://yoursite.com/2018/04/02/机器学习知识总结/index.html">
<meta property="og:site_name" content="飞白">
<meta property="og:description" content="过拟合与欠拟合">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/mubiaohanshu.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/duiouwenti.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/tu.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/smo1.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/duishusunshihanshu.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/pingfanggenwucha.png">
<meta property="og:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/zhongweishuwucha.png">
<meta property="og:updated_time" content="2018-04-05T14:23:52.248Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习知识总结">
<meta name="twitter:description" content="过拟合与欠拟合">
<meta name="twitter:image" content="http://yoursite.com/2018/04/02/机器学习知识总结/mubiaohanshu.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/04/02/机器学习知识总结/"/>





  <title>机器学习知识总结 | 飞白</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">飞白</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/02/机器学习知识总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="飞白">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习知识总结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-02T08:48:43+08:00">
                2018-04-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h2><a id="more"></a>
<p>拟合程度的好与坏实际上是看模型在训练集上和测试集上的误差，训练集上误差越小同时测试集合训练集误差接近这两个条件都满足时，模型才足够好。<br>而模型的泛化误差又可以分为bias和variance两部分。<br>首先如果我们能够获得所有可能的数据集合，并在这个数据集合上将loss最小化，这样学习到的模型就可以称之为“真实模型”，当然，我们是无论如何都不能获得并训练所有可能的数据的，所以“真实模型”肯定存在，但无法获得，我们的最终目标就是去学习一个模型使其更加接近这个真实模型。而bias和variance分别从两个方面来描述了我们学习到的模型与真实模型之间的差距。Bias是 “用所有可能的训练数据集训练出的所有模型的输出的平均值” 与 “真实模型”的输出值之间的差异；Variance则是“不同的训练数据集训练出的模型”的输出值之间的差异。</p>
<p>关于cross validation中k值对bias和variance的影响，那我就从其他方面来举个例子。假设我们现在有一组训练数据，需要训练一个模型（基于梯度的学习，不包括最近邻等方法）。在训练过程的最初，bias很大，因为我们的模型还没有来得及开始学习，也就是与“真实模型”差距很大。然而此时variance却很小，因为训练数据集（training data）还没有来得及对模型产生影响，所以此时将模型应用于“不同的”训练数据集也不会有太大差异。而随着训练过程的进行，bias变小了，因为我们的模型变得“聪明”了，懂得了更多关于“真实模型”的信息，输出值与真实值之间更加接近了。但是如果我们训练得时间太久了，variance就会变得很大，因为我们除了学习到关于真实模型的信息，还学到了许多具体的，只针对我们使用的训练集（真实数据的子集）的信息。而不同的可能训练数据集（真实数据的子集）之间的某些特征和噪声是不一致的，这就导致了我们的模型在很多其他的数据集上就无法获得很好的效果，也就是所谓的overfitting（过学习）。</p>
<p>1.对于树模型，模型的复杂程度与节点的数量有关<br>防止过拟合：<br>    ①树过于大之前便停止生长<br>    ·每个页中至少需要多少个数据（threshold)<br>    ·如何判断这个阈值（假设检验/P-值）<br>    ②等树生长到足够大之后进行修剪</p>
<p>2.对于数值模型问题，变量的数量与模型的复杂度有关。<br>    二位情况下，两个点可以用一条直线拟合<br>    三维情况下，三个点可以用一个平面拟合<br>    ……<br>    随着维度增加，可以拟合任意数量的点<br>    ①正则化：L1,L2<br>    ②dropout<br>    ③early stop：结合cross validation使用<br>    ④cross validation：数据量较小时，用这个方法比较好。<br>    当K值大的时候， 我们会有更少的Bias(偏差), 更多的Variance。<br>    当K值小的时候， 我们会有更多的Bias(偏差), 更少的Variance。<br>    ⑤增大数据集</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>因为有些数据集是线性不可分的，所以通过引入激活函数来加入非线性因素的。<br>1.sigmod函数<br>也叫Logistic函数，取值范围（0，1），它可以将一个实数映射到(0,1)的区间，可以用来做二分类。<br>sigmoid缺点：<br>激活函数计算量大，反向传播求误差梯度时，求导涉及除法<br>反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练（sigmoid的饱和性）<br>此外，sigmoid函数的输出均大于0，使得输出不是0均值，这称为偏移现象，这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。<br>2.tanh函数<br>与 sigmoid 的区别是，tanh 是 0 均值的，因此实际应用中 tanh 会比 sigmoid 更好，然而，tanh一样具有软饱和性，从而造成梯度消失。<br>3.relu函数<br>ReLU 的优点：<br>Krizhevsky et al. 发现使用 ReLU 得到的 SGD 的收敛速度会比 sigmoid/tanh 快很多。除此之外，当x&lt;0时，ReLU硬饱和，而当x&gt;0时，则不存在饱和问题。所以，ReLU 能够在x&gt;0时保持梯度不衰减，从而缓解梯度消失问题。这让我们能够直接以监督的方式训练深度神经网络，而无需依赖无监督的逐层预训练。</p>
<p>ReLU 的缺点：<br>随着训练的推进，部分输入会落入硬饱和区，导致对应权重无法更新。这种现象被称为“神经元死亡”。与sigmoid类似，ReLU的输出均值也大于0，偏移现象和 神经元死亡会共同影响网络的收敛性。<br>4.softmax函数<br>sigmoid将一个real value映射到（0,1）的区间，用来做二分类。</p>
<p>而 softmax 把一个 k 维的real value向量（a1,a2,a3,a4….）映射成一个（b1,b2,b3,b4….）其中 bi 是一个 0～1 的常数，输出神经元之和为 1.0，所以相当于概率值，然后可以根据 bi 的概率大小来进行多分类的任务。</p>
<h2 id="梯度爆炸与梯度消失"><a href="#梯度爆炸与梯度消失" class="headerlink" title="梯度爆炸与梯度消失"></a>梯度爆炸与梯度消失</h2><p>1.梯度消失：因为通常神经网络所用的激活函数是sigmoid函数，这个函数有个特点，就是能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是f′(x)=f(x)(1−f(x))。因此两个0到1之间的数相乘，得到的结果就会变得很小了。神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新，这就是梯度消失。<br>2.梯度爆炸：梯度爆炸就是由于初始化权值过大，前面层会比后面层变化的更快，就会导致权值越来越大，梯度爆炸的现象就发生了。</p>
<p>在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。</p>
<p>网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。<br>3.如何确定是否出现梯度爆炸？</p>
<p>训练过程中出现梯度爆炸会伴随一些细微的信号，如：</p>
<p>模型无法从训练数据中获得更新（如低损失）。</p>
<p>模型不稳定，导致更新过程中的损失出现显著变化。</p>
<p>训练过程中，模型损失变成 NaN。<br>4.如何修复梯度爆炸问题？<br>①重新设计网络模型<br>②使用Relu激活函数<br>③使用长短期记忆网络<br>④使用梯度截断<br>⑤使用权重正则化：检查网络权重的大小，并惩罚产生较大权重值的损失函数</p>
<h2 id="分类算法（LR-SVM-朴素贝叶斯-决策树，RF-GBDT-Adaboost-xgboost，KNN"><a href="#分类算法（LR-SVM-朴素贝叶斯-决策树，RF-GBDT-Adaboost-xgboost，KNN" class="headerlink" title="分类算法（LR,SVM,朴素贝叶斯,决策树，RF,GBDT,Adaboost,xgboost，KNN)"></a>分类算法（LR,SVM,朴素贝叶斯,决策树，RF,GBDT,Adaboost,xgboost，KNN)</h2><h3 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h3><p>1.面对一个回归或者分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏。逻辑回归函数（Sigmod)非凸函数，是用最大似然估计进行模型参数的求解。<br>2.Regression 常规步骤<br>寻找h函数（即预测函数）<br>构造J函数（损失函数）<br>想办法使得J函数最小并求得回归参数（θ）<br>3.优缺点<br>优点：<br>1）速度快，适合二分类问题<br>2）简单易于理解，直接看到各个特征的权重<br>3）容易使用和解释，直接看到各个特征的权重。<br>缺点：<br>对数据和场景的适应能力有局限性，不如决策树算法适应性那么强</p>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>SVM是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大是它有别于感知机）</p>
<p>（1）当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；</p>
<p>（2）当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；</p>
<p>（3）当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。</p>
<p>注：以上各SVM的数学推导应该熟悉：硬间隔最大化（几何间隔）—学习的对偶问题—软间隔最大化（引入松弛变量）—非线性支持向量机（核技巧）。</p>
<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>1.优点：可以解决小样本下机器学习的问题。提高泛化性能。避免神经网络结构选择和局部极小的问题。<br>2.缺点：缺失数据敏感。内存消耗大，难以解释。</p>
<h4 id="SVM为什么采用间隔最大化？"><a href="#SVM为什么采用间隔最大化？" class="headerlink" title="SVM为什么采用间隔最大化？"></a>SVM为什么采用间隔最大化？</h4><p>当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。</p>
<p>感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。</p>
<p>线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。</p>
<p>然后应该借此阐述，几何间隔，函数间隔，及从函数间隔—&gt;求解最小化1/2 ||w||^2 时的w和b。即线性可分支持向量机学习算法—最大间隔法的由来。</p>
<h4 id="线性SVM"><a href="#线性SVM" class="headerlink" title="线性SVM"></a>线性SVM</h4><p>加入了松弛因子。<br>SVM目标函数为<img src="/2018/04/02/机器学习知识总结/mubiaohanshu.png" alt="lbxx"><br>而损失函数就是后面的Ei的和<br>SVM实际上可以看作是hinge损失和l2正则</p>
<h4 id="解释对偶的概念。"><a href="#解释对偶的概念。" class="headerlink" title="解释对偶的概念。"></a>解释对偶的概念。</h4><p>极小极大问题转化为极大极小问题。SVM拉格朗日乘子法前半部分1/2||w^2||为二次函数，凸函数，后半部分线性的，所以转为为对偶问题是充分条件。一般情况下极小极大大于等于极大极小。</p>
<h4 id="为什么要将求解SVM的原始问题转换为其对偶问题？"><a href="#为什么要将求解SVM的原始问题转换为其对偶问题？" class="headerlink" title="为什么要将求解SVM的原始问题转换为其对偶问题？"></a>为什么要将求解SVM的原始问题转换为其对偶问题？</h4><p>一、是对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）</p>
<p>二、自然引入核函数，进而推广到非线性分类问题。</p>
<h4 id="为什么SVM要引入核函数？"><a href="#为什么SVM要引入核函数？" class="headerlink" title="为什么SVM要引入核函数？"></a>为什么SVM要引入核函数？</h4><p>当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。</p>
<p>引入映射后的对偶问题：<br><img src="/2018/04/02/机器学习知识总结/duiouwenti.png" alt="lbxx"><br>在学习预测中，只定义核函数K(x,y)，而不是显式的定义映射函数ϕ。因为特征空间维数可能很高，甚至可能是无穷维，因此直接计算ϕ(x)·ϕ(y)是比较困难的。相反，直接计算K(x,y)比较容易（即直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果）。</p>
<p>核函数的定义：K(x,y)=&lt;ϕ(x),ϕ(y)&gt;，即在特征空间的内积等于它们在原始样本空间中通过核函数K计算的结果。</p>
<p>除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。</p>
<h4 id="为什么SVM对缺失数据敏感？"><a href="#为什么SVM对缺失数据敏感？" class="headerlink" title="为什么SVM对缺失数据敏感？"></a>为什么SVM对缺失数据敏感？</h4><p>这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM没有处理缺失值的策略（决策树有）。而SVM希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。</p>
<h4 id="SVM如何处理多分类问题？"><a href="#SVM如何处理多分类问题？" class="headerlink" title="SVM如何处理多分类问题？"></a>SVM如何处理多分类问题？</h4><p>一般有两种做法：一种是直接法，直接在目标函数上修改，将多个分类面的参数求解合并到一个最优化问题里面。看似简单但是计算量却非常的大。</p>
<p>另外一种做法是间接法：对训练器进行组合。其中比较典型的有一对一，和一对多。</p>
<p>一对多，就是对每个类都训练出一个分类器，由svm是二分类，所以将此而分类器的两类设定为目标类为一类，其余类为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，那个分类器的概率高，那么这个样本就属于哪一类。这种方法效果不太好，bias比较高。</p>
<p>svm一对一法（one-vs-one），针对任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k) 个分类器，这样当有一个新的样本要来的时候，用这C(2,k) 个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。</p>
<h4 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h4><p>KKT条件的定理是什么呢？就是如果一个优化问题在转变完后变成<br>L(x,α,β)=f(x)+∑αigi(x)+∑βihi(x)<br>L(x,α,β)=f(x)+∑αigi(x)+∑βihi(x)</p>
<p>其中g是不等式约束，h是等式约束（像上面那个只有不等式约束，也可能有等式约束）。那么KKT条件就是函数的最优值必定满足下面条件：<br>(1) L对各个x求导为零；<br>(2) h(x)=0;<br>(3) ∑αigi(x)=0，αi≥0</p>
<h4 id="SMO-序列最小最优化算法"><a href="#SMO-序列最小最优化算法" class="headerlink" title="SMO(序列最小最优化算法)"></a>SMO(序列最小最优化算法)</h4><p>1.定义：用于求解SVM对偶问题的最优解的二次规划方法。<br>2.<img src="/2018/04/02/机器学习知识总结/tu.png" alt="lbxx"><br>为了求解N个参数(α1,α2,α3,…,αN)，首先想到的是坐标上升的思路，例如求解α1,可以固定其他N-1个参数，可以看成关于α1的一元函数求解，但是注意到上述问题的等式约束条件<br><img src="/2018/04/02/机器学习知识总结/smo1.png" alt="lbxx">当固定其他参数时，参数α1也被固定，因此此种方法不可用。<br>SMO算法选择同时优化两个参数，固定其他N-2个参数，假设选择的变量为α1,α2,固定其他参数α3,α4,…,αN,由于参数α3,α4,…,αN的固定，可以简化目标函数为只关于α1,α2的二元函数，Constant表示常数项(不包含变量α1,α2的项)。 </p>
<h4 id="python实现代码"><a href="#python实现代码" class="headerlink" title="python实现代码"></a>python实现代码</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">import random </span><br><span class="line"></span><br><span class="line">def loadDataSet(fileName):  <span class="comment">#构建数据库和标记库</span></span><br><span class="line">    dataMat = []; labelMat = [] </span><br><span class="line">    fr = open(fileName) </span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines(): </span><br><span class="line">        lineArr = line.strip().split(<span class="string">'\t'</span>) </span><br><span class="line">        dataMat.append([<span class="built_in">float</span>(lineArr[0]), <span class="built_in">float</span>(lineArr[1])]) </span><br><span class="line">        labelMat.append(<span class="built_in">float</span>(lineArr[2]))  <span class="comment">#只有一列</span></span><br><span class="line">    <span class="built_in">return</span> dataMat, labelMat </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def selectJrand(i, m):  <span class="comment">#生成一个随机数</span></span><br><span class="line">    j=i </span><br><span class="line">    <span class="keyword">while</span>(j==i): </span><br><span class="line">        j=int(random.uniform(0, m))  <span class="comment">#生成一个[0, m]的随机数，int转换为整数。注意，需要import random</span></span><br><span class="line">    <span class="built_in">return</span> j </span><br><span class="line"></span><br><span class="line">def clipAlpha(aj, H, L):  <span class="comment">#阈值函数</span></span><br><span class="line">    <span class="keyword">if</span> aj&gt;H: </span><br><span class="line">        aj=H </span><br><span class="line">    <span class="keyword">if</span> aj&lt;L: </span><br><span class="line">        aj=L </span><br><span class="line">    <span class="built_in">return</span> aj </span><br><span class="line"></span><br><span class="line">def smoSimple(dataMatIn, classLabels, C, toler, maxIter): </span><br><span class="line">    dataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose()</span><br><span class="line">    b = 0; m,n = shape(dataMatrix)</span><br><span class="line">    alphas = mat(zeros((m,1)))</span><br><span class="line">    iter = 0</span><br><span class="line">    <span class="keyword">while</span>(iter&lt;maxIter):  <span class="comment">#迭代次数</span></span><br><span class="line">        alphaPairsChanged=0 </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):  <span class="comment">#在数据集上遍历每一个alpha</span></span><br><span class="line">            <span class="comment">#print alphas </span></span><br><span class="line">            <span class="comment">#print labelMat</span></span><br><span class="line">            fXi = <span class="built_in">float</span>(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b</span><br><span class="line">            <span class="comment">#fXi=float(np.multiply(alphas, labelMat).T*dataMatrix*dataMatrix[i, :].T)+b  #.T也是转置</span></span><br><span class="line">            Ei=fXi-float(labelMat[i]) </span><br><span class="line">            <span class="keyword">if</span>((labelMat[i]*Ei&lt;-toler) and (alphas[i]&lt;C)) or ((labelMat[i]*Ei&gt;toler) and (alphas[i]&gt;0)): </span><br><span class="line">                j=selectJrand(i, m)  <span class="comment">#从m中选择一个随机数，第2个alpha j</span></span><br><span class="line">                fXj=<span class="built_in">float</span>(multiply(alphas, labelMat).T*dataMatrix*dataMatrix[j, :].T)+b </span><br><span class="line">                Ej=fXj-float(labelMat[j]) </span><br><span class="line">                </span><br><span class="line">                alphaIold=alphas[i].copy()  <span class="comment">#复制下来，便于比较</span></span><br><span class="line">                alphaJold=alphas[j].copy() </span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span>(labelMat[i]!=labelMat[j]):  <span class="comment">#开始计算L和H</span></span><br><span class="line">                    L=max(0, alphas[j]-alphas[i]) </span><br><span class="line">                    H=min(C, C+alphas[j]-alphas[i]) </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    L=max(0, alphas[j]+alphas[i]-C) </span><br><span class="line">                    H=min(C, alphas[j]+alphas[i]) </span><br><span class="line">                <span class="keyword">if</span> L==H: </span><br><span class="line">                    <span class="built_in">print</span> <span class="string">'L==H'</span> </span><br><span class="line">                    <span class="built_in">continue</span> </span><br><span class="line">                </span><br><span class="line">                <span class="comment">#eta是alphas[j]的最优修改量，如果eta为零，退出for当前循环</span></span><br><span class="line">                eta=2.0*dataMatrix[i, :]*dataMatrix[j, :].T-\</span><br><span class="line">                    dataMatrix[i, :]*dataMatrix[i, :].T-\</span><br><span class="line">                    dataMatrix[j, :]*dataMatrix[j, :].T </span><br><span class="line">                <span class="keyword">if</span> eta&gt;=0: </span><br><span class="line">                    <span class="built_in">print</span> <span class="string">'eta&gt;=0'</span> </span><br><span class="line">                    <span class="built_in">continue</span> </span><br><span class="line">                alphas[j]-=labelMat[j]*(Ei-Ej)/eta  <span class="comment">#调整alphas[j] </span></span><br><span class="line">                alphas[j]=clipAlpha(alphas[j], H, L)  </span><br><span class="line">                <span class="keyword">if</span>(abs(alphas[j]-alphaJold)&lt;0.00001):  <span class="comment">#如果alphas[j]没有调整</span></span><br><span class="line">                    <span class="built_in">print</span> <span class="string">'j not moving enough'</span> </span><br><span class="line">                    <span class="built_in">continue</span> </span><br><span class="line">                alphas[i]+=labelMat[j]*labelMat[i]*(alphaJold-alphas[j])  <span class="comment">#调整alphas[i]</span></span><br><span class="line">                b1=b-Ei-labelMat[i]*(alphas[i]-alphaIold)*\</span><br><span class="line">                    dataMatrix[i, :]*dataMatrix[i, :].T-\</span><br><span class="line">                    labelMat[j]*(alphas[j]-alphaJold)*\</span><br><span class="line">                    dataMatrix[i, :]*dataMatrix[j, :].T </span><br><span class="line">                b2=b-Ej-labelMat[i]*(alphas[i]-alphaIold)*\</span><br><span class="line">                    dataMatrix[i, :]*dataMatrix[j, :].T-\</span><br><span class="line">                    labelMat[j]*(alphas[j]-alphaJold)*\</span><br><span class="line">                    dataMatrix[j, :]*dataMatrix[j, :].T </span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span>(0&lt;alphas[i]) and (C&gt;alphas[i]): </span><br><span class="line">                    b=b1 </span><br><span class="line">                <span class="keyword">elif</span>(0&lt;alphas[j]) and (C&gt;alphas[j]): </span><br><span class="line">                    b=b2 </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    b=(b1+b2)/2.0 </span><br><span class="line">                alphaPairsChanged+=1 </span><br><span class="line">                </span><br><span class="line">                <span class="built_in">print</span> <span class="string">'iter: %d i: %d, pairs changed %d'</span> %(iter, i, alphaPairsChanged) </span><br><span class="line">        <span class="keyword">if</span>(alphaPairsChanged==0): </span><br><span class="line">            iter+=1 </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            iter=0 </span><br><span class="line">        <span class="built_in">print</span> <span class="string">'iteration number: %d'</span> %iter</span><br><span class="line">    <span class="built_in">return</span> b, alphas </span><br><span class="line"></span><br><span class="line">                </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>: </span><br><span class="line">    dataArr, labelArr=loadDataSet(<span class="string">'testSet.txt'</span>) </span><br><span class="line">    b, alphas=smoSimple(dataArr, labelArr, 0.6, 0.001, 40)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span> b, alphas</span><br></pre></td></tr></table></figure>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>1.基本描述：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此分类项属于哪个类别。<br>2.优点：方法简单，分类准确率高，速度快，所需估计的参数少，对于缺失数据不敏感。<br>3.缺点：假设一个属性对定类的影响独立于其他的属性值，这往往并不成立。<br>4.</p>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="基本描述"><a href="#基本描述" class="headerlink" title="基本描述"></a>基本描述</h4><p>决策树是一种简单但广泛使用的分类器，它通过训练数据构建决策树，对未知的数据进行分类。决策树的每个内部节点表示在一个属性上的测试，每个分枝代表该测试的一个输出，而每个叶结点存放着一个类标号。</p>
<p>在决策树算法中，ID3基于信息增益作为属性选择的度量，C4.5基于信息增益比作为属性选择的度量，CART基于基尼指数作为属性选择的度量。</p>
<h4 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h4><p>优点：不需要任何领域知识或参数假设。适合高维数据。简单易于理解，可解释性强。短时间内处理大量数据，得到可行且效果较好的结果(训练速度快)。<br>缺点：对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。<br>易于过拟合。忽略属性之间的相关性。</p>
<h3 id="RF"><a href="#RF" class="headerlink" title="RF"></a>RF</h3><p>1.数据的随机选取：<br>第一，从原始的数据集中采取有放回的抽样，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。<br>第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。<br>2.待选特征的随机选取：<br>与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。这样能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。</p>
<h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><p>1.基本描述：通过计算每个训练样例到待分类样品的距离，取和待分类样品距离最近的K个训练样例，K个样品中哪个类别的训练样例占多数，则待分类样品就属于哪个类别。<br>2.优缺点<br>优点：适用于样本容量比较大的分类问题<br>缺点：计算量太大，对于样本量较小的分类问题，会产生误分。</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p>1.通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。<br>2.如何选择特征：原始的gbdt的做法非常的暴力，首先遍历每个特征，然后对每个特征遍历它所有可能的切分点，找到最优特征 m 的最优切分点 j。</p>
<h3 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h3><h3 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h3><p>（1）xgboost在代价函数里自带加入了正则项，用于控制模型的复杂度。<br>（2）xgboost在进行节点的分裂时，支持各个特征多线程进行增益计算，因此算法更快，准确率也相对高一些。</p>
<h3 id="GBDT-和随机森林的区别？"><a href="#GBDT-和随机森林的区别？" class="headerlink" title="GBDT 和随机森林的区别？"></a>GBDT 和随机森林的区别？</h3><h3 id="如何判断函数凸或非凸？什么是凸优化？"><a href="#如何判断函数凸或非凸？什么是凸优化？" class="headerlink" title="如何判断函数凸或非凸？什么是凸优化？"></a>如何判断函数凸或非凸？什么是凸优化？</h3><h3 id="如何解决类别不平衡问题？"><a href="#如何解决类别不平衡问题？" class="headerlink" title="如何解决类别不平衡问题？"></a>如何解决类别不平衡问题？</h3><h3 id="如何进行特征选择？"><a href="#如何进行特征选择？" class="headerlink" title="如何进行特征选择？"></a>如何进行特征选择？</h3><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><h3 id="分类评价指标"><a href="#分类评价指标" class="headerlink" title="分类评价指标"></a>分类评价指标</h3><p>1.准确率<br>2.平均准确率<br>计算每个类别的准确率最后取平均<br>3.对数损失函数<br>在分类输出中，若输出不再是0-1，而是实数值，即属于每个类别的概率，那么可以使用Log-loss对分类结果进行评价。这个输出概率表示该记录所属的其对应的类别的置信度。比如如果样本本属于类别0，但是分类器则输出其属于类别1的概率为0.51，那么这种情况认为分类器出错了。该概率接近了分类器的分类的边界概率0.5。Log-loss是一个软的分类准确率度量方法，使用概率来表示其所属的类别的置信度。Log-loss具体的数学表达式为：<br><img src="/2018/04/02/机器学习知识总结/duishusunshihanshu.png" alt="lbxx"><br>其中，yiyi是指第ii个样本所属的真实类别0或者1，pipi表示第ii个样本属于类别1的概率，这样上式中的两个部分对于每个样本只会选择其一，因为有一个一定为0，当预测与实际类别完全匹配时，则两个部分都是0，其中假定0log0=0。<br>4.精确率，召回率，F-measure<br>5.AUC：即曲线下的面积，这条曲线便是ROC曲线</p>
<h3 id="回归评价指标"><a href="#回归评价指标" class="headerlink" title="回归评价指标"></a>回归评价指标</h3><p>与分类不同的是，回归是对连续的实数值进行预测，即输出值是连续的实数值，而分类中是离散值。例如，给你历史股票价格，公司与市场的一些信息，需要你去预测将来一段时间内股票的价格走势。那么这个任务便是回归任务。对于回归模型的评价指标主要有以下几种：<br>1.RMSE:回归模型中最常用的评价模型便是RMSE（root mean square error，平方根误差），其又被称为RMSD（root mean square deviation），其定义如下：<br><img src="/2018/04/02/机器学习知识总结/pingfanggenwucha.png" alt="lbxx"><br>其中，yi是第i个样本的真实值，yi^是第ii个样本的预测值，n是样本的个数。该评价指标使用的便是欧式距离。<br>RMSE虽然广为使用，但是其存在一些缺点，因为它是使用平均误差，而平均值对异常点（outliers）较敏感，如果回归器对某个点的回归值很不理性，那么它的误差则较大，从而会对RMSE的值有较大影响，即平均值是非鲁棒的。<br>2.Quantiles of Errors<br>在现实数据中，往往会存在异常点，并且模型可能对异常点拟合得并不好，因此提高评价指标的鲁棒性至关重要，于是可以使用中位数来替代平均数，如MAPE：<br><img src="/2018/04/02/机器学习知识总结/zhongweishuwucha.png" alt="lbxx"><br>MAPE是一个相对误差的中位数，当然也可以使用别的分位数。<br>有时我们可以使用相对误差不超过设定的值来计算平均误差，如当|yi−yi^|/yi超过100%（具体的值要根据问题的实际情况）则认为其是一个异常点，，从而剔除这个异常点，将异常点剔除之后，再计算平均误差或者中位数误差来对模型进行评价。</p>
<h3 id="排序评价指标"><a href="#排序评价指标" class="headerlink" title="排序评价指标"></a>排序评价指标</h3><p>1.精确率-召回率-F-measure</p>
<h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><p>1.什么是模型超参数？<br>模型超参数是模型外部的配置，其值不能从数据估计得到。<br>2.具体特征有：<br>模型超参数常应用于估计模型参数的过程中。<br>模型超参数通常由实践者直接指定。<br>模型超参数通常可以使用启发式方法来设置。<br>模型超参数通常根据给定的预测建模问题而调整。<br>3.例子：<br>训练神经网络的学习速率。<br>支持向量机的C和sigma超参数。<br>k邻域中的k。<br>4.网格搜索，随机搜索，贝叶斯优化<br><a href="https://blog.csdn.net/gzj533/article/details/77734310" target="_blank" rel="noopener">更多</a></p>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><h2 id="局部敏感哈希"><a href="#局部敏感哈希" class="headerlink" title="局部敏感哈希"></a>局部敏感哈希</h2><h2 id="数据偏斜"><a href="#数据偏斜" class="headerlink" title="数据偏斜"></a>数据偏斜</h2><p>基于算法：<br>在算法中提高少数类别样例的权重，对少数类别样本错误分类的代价高于多数类别样本的错分。</p>
<p>基于数据：<br>两种。一是过采样，在数据集中增加少数类别样例。二是下采样，减少多数类别的样本个数。</p>
<h3 id="重采样"><a href="#重采样" class="headerlink" title="重采样"></a>重采样</h3><p>上采样<br>下采样</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/01/算法/" rel="next" title="算法">
                <i class="fa fa-chevron-left"></i> 算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/05/深度学习知识总结/" rel="prev" title="深度学习知识总结">
                深度学习知识总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John</p>
              <p class="site-description motion-element" itemprop="description">学无止境</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#过拟合与欠拟合"><span class="nav-number">1.</span> <span class="nav-text">过拟合与欠拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数"><span class="nav-number">2.</span> <span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度爆炸与梯度消失"><span class="nav-number">3.</span> <span class="nav-text">梯度爆炸与梯度消失</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分类算法（LR-SVM-朴素贝叶斯-决策树，RF-GBDT-Adaboost-xgboost，KNN"><span class="nav-number">4.</span> <span class="nav-text">分类算法（LR,SVM,朴素贝叶斯,决策树，RF,GBDT,Adaboost,xgboost，KNN)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LR"><span class="nav-number">4.1.</span> <span class="nav-text">LR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM"><span class="nav-number">4.2.</span> <span class="nav-text">SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#定义"><span class="nav-number">4.2.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优缺点"><span class="nav-number">4.2.2.</span> <span class="nav-text">优缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM为什么采用间隔最大化？"><span class="nav-number">4.2.3.</span> <span class="nav-text">SVM为什么采用间隔最大化？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性SVM"><span class="nav-number">4.2.4.</span> <span class="nav-text">线性SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解释对偶的概念。"><span class="nav-number">4.2.5.</span> <span class="nav-text">解释对偶的概念。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么要将求解SVM的原始问题转换为其对偶问题？"><span class="nav-number">4.2.6.</span> <span class="nav-text">为什么要将求解SVM的原始问题转换为其对偶问题？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么SVM要引入核函数？"><span class="nav-number">4.2.7.</span> <span class="nav-text">为什么SVM要引入核函数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么SVM对缺失数据敏感？"><span class="nav-number">4.2.8.</span> <span class="nav-text">为什么SVM对缺失数据敏感？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM如何处理多分类问题？"><span class="nav-number">4.2.9.</span> <span class="nav-text">SVM如何处理多分类问题？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KKT条件"><span class="nav-number">4.2.10.</span> <span class="nav-text">KKT条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SMO-序列最小最优化算法"><span class="nav-number">4.2.11.</span> <span class="nav-text">SMO(序列最小最优化算法)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#python实现代码"><span class="nav-number">4.2.12.</span> <span class="nav-text">python实现代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#朴素贝叶斯"><span class="nav-number">4.3.</span> <span class="nav-text">朴素贝叶斯</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树"><span class="nav-number">4.4.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本描述"><span class="nav-number">4.4.1.</span> <span class="nav-text">基本描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优缺点-1"><span class="nav-number">4.4.2.</span> <span class="nav-text">优缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RF"><span class="nav-number">4.5.</span> <span class="nav-text">RF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KNN"><span class="nav-number">4.6.</span> <span class="nav-text">KNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT"><span class="nav-number">4.7.</span> <span class="nav-text">GBDT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adaboost"><span class="nav-number">4.8.</span> <span class="nav-text">Adaboost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xgboost"><span class="nav-number">4.9.</span> <span class="nav-text">xgboost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GBDT-和随机森林的区别？"><span class="nav-number">4.10.</span> <span class="nav-text">GBDT 和随机森林的区别？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何判断函数凸或非凸？什么是凸优化？"><span class="nav-number">4.11.</span> <span class="nav-text">如何判断函数凸或非凸？什么是凸优化？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何解决类别不平衡问题？"><span class="nav-number">4.12.</span> <span class="nav-text">如何解决类别不平衡问题？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何进行特征选择？"><span class="nav-number">4.13.</span> <span class="nav-text">如何进行特征选择？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#评价指标"><span class="nav-number">5.</span> <span class="nav-text">评价指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分类评价指标"><span class="nav-number">5.1.</span> <span class="nav-text">分类评价指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#回归评价指标"><span class="nav-number">5.2.</span> <span class="nav-text">回归评价指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#排序评价指标"><span class="nav-number">5.3.</span> <span class="nav-text">排序评价指标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#超参数"><span class="nav-number">6.</span> <span class="nav-text">超参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#聚类"><span class="nav-number">7.</span> <span class="nav-text">聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#局部敏感哈希"><span class="nav-number">8.</span> <span class="nav-text">局部敏感哈希</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据偏斜"><span class="nav-number">9.</span> <span class="nav-text">数据偏斜</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#重采样"><span class="nav-number">9.1.</span> <span class="nav-text">重采样</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
